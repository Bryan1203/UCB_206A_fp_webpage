<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PRISM: Precision Relocation via Intelligent Slide Manipulation</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <!-- Navigation -->
    <nav id="navbar">
        <div class="nav-container">
            <div class="logo">PRISM</div>
            <ul class="nav-menu">
                <li><a href="#introduction">Introduction</a></li>
                <li><a href="#results">Results</a></li>
                <li><a href="#design">Design</a></li>
                <li><a href="#implementation">Implementation</a></li>
                <li><a href="#conclusion">Conclusion</a></li>
                <li><a href="#team">Team</a></li>
            </ul>
        </div>
    </nav>

    <!-- Hero Section -->
    <section id="home" class="hero">
        <div class="hero-content">
            <h1 class="glitch" data-text="PRISM">PRISM</h1>
            <h2>Precision Relocation via Intelligent Slide Manipulation</h2>
            <p class="subtitle">An intelligent robotic system for automated slide handling</p>
            <div class="hero-meta">
                <span class="course">EECS/ME 206A - Team 45</span>
                <span class="separator">|</span>
                <span class="sponsor">Industry Project by Ember Robotics</span>
            </div>
            <a href="#introduction" class="cta-button">Explore Project</a>
        </div>
    </section>

    <!-- Problem Statement -->
    <section id="introduction" class="section problem-section">
        <div class="container">
            <h2 class="section-title">Introduction</h2>
            <p class="section-note">This project is an Industry Track collaboration with Ember Robotics to design and implement a vision-guided pick-and-place system using a Techman robotic arm. Our goal is to manipulate fragile glass lab slides with high precision, which requires robust object detection, accurate motion planning, and precise actuation. More importantly, we will integrate the Ember platform for real-time performance monitoring, analyzing joint and motor data to detect anomalies such as slippage or misalignment. The outcome will be a data-driven, self-observing robotic system that enhances reliability and minimizes downtime, showcasing advanced robotics for industrial applications.</p>
            <div class="problem-grid">
                <div class="problem-card">
                    <div class="icon">üë∑‚Äç‚ôÇÔ∏è</div>
                    <h3>Manual & Inefficient</h3>
                    <p>Current slide handling workflow requires manual intervention</p>
                </div>
                <div class="problem-card">
                    <div class="icon">ü¶†</div>
                    <h3>Contamination Risks</h3>
                    <p>Potential contamination risks that affect experiment results</p>
                </div>
            </div>

            <div class="goal-section">
                <div class="goal-inner">
                    <div class="goal-content">
                        <div class="goal-text">
                            <h3>Our Goal</h3>
                            <p>Our goal is to design, build, and test a vision-guided pick-and-place system using a Techman robotic arm that can manipulate lab glass slides with high precision while being continuously monitored for performance degradations or system anomalies. For successful task execution, the robot should detect the location of each slide, pick it from a tray, and place it accurately onto a target rack.</p>
                            <p>Design an intelligent manipulator system with:</p>
                            <ul class="feature-list">
                                <li>Computer vision for slide identification</li>
                                <li>Precise pick-and-place in 3D space</li>
                                <li>Robust operation for continuous tasks</li>
                            </ul>
                        </div>
                        <div class="goal-graphic">
                            <img src="images/image23.png" alt="PRISM goal illustration">
                        </div>
                    </div>
                </div>
                <p class="approach">Our approach combines computer vision, motion planning, and custom hardware into a complete ROS2 system. We detect slide poses, compute precise trajectories, and execute robust pick-and-place actions with a custom adapter and gripper, targeting repeatable performance in a laboratory setting.</p>
            </div>

        </div>
    </section>

    <!-- Demo Video -->
    <section id="results" class="section">
        <div class="container">
            <h2 class="section-title">Results</h2>
            <p class="section-note">The final system demonstrates consistent and repeatable slide manipulation across multiple trials, as shown in the visual demonstrations below.</p>
            <div class="video-wrapper">
                <iframe src="https://www.youtube.com/embed/4Mbxl2Z15pg?si=uDb3a2DtugLRp3_x"
                        title="PRISM Demo Video"
                        frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                        allowfullscreen>
                </iframe>
            </div>
        </div>
    </section>

    <!-- System Overview -->
    <section id="design" class="section dark-section">
        <div class="container">
            <h2 class="section-title">Design</h2>
            <p class="section-note">The system is designed as a modular pipeline consisting of perception, planning, and actuation components. Hardware design focuses on a custom adapter to ensure mechanical stability, while software design emphasizes clear separation between sensing and execution through iterative refinement.</p>
            <div class="system-pillars">
                <a class="pillar-link" href="#perception-target">
                    <div class="pillar">
                        <div class="pillar-icon">üëÅÔ∏è</div>
                        <h3>Perception</h3>
                        <p>Computer Vision algorithm to detect slides from RGB image and estimate their 6D poses</p>
                    </div>
                </a>
                <a class="pillar-link" href="#planning">
                    <div class="pillar">
                        <div class="pillar-icon">üß†</div>
                        <h3>Planning</h3>
                        <p>Motion planning with MoveIt2, leveraging OMPL for path-finding and IK solvers</p>
                    </div>
                </a>
                <a class="pillar-link" href="#actuation">
                    <div class="pillar">
                        <div class="pillar-icon">‚öôÔ∏è</div>
                        <h3>Actuation</h3>
                        <p>Custom hardware interface executing trajectories via ROS2 services on microcontroller-driven gripper</p>
                    </div>
                </a>
            </div>
        </div>
    </section>

    <!-- System Architecture -->
    <section id="architecture" class="section dark-section">
        <div class="container">
            <h2 class="section-title">System Architecture</h2>

            <div class="architecture-image-container">
                <img src="images/42A5B40D-EB54-408C-9088-F089410A0A6A.PNG" alt="PRISM System Architecture" class="architecture-main-image">
            </div>

        </div>
    </section>

    <!-- Perception Section -->
    <section id="implementation" class="section">
        <div class="container">
            <h2 class="section-title">Implementation</h2>
            <p class="section-note">The implementation integrates custom hardware with a software pipeline for perception, motion planning, and execution, enabling reliable end-to-end slide manipulation.</p>

            <div id="perception-target" class="anchor-spacer"></div>
            <h3 class="section-subhead">Perception</h3>
            <p class="section-note">The perception module is responsible for detecting the microscope slide and estimating its pose within the workspace. Using Intel RealSense input, the system identifies the slide‚Äôs position and orientation relative to the robot with CV algorithm, providing a consistent geometric reference for downstream modules. Accurate perception is critical for reliable grasping, as small errors in pose estimation can lead to failed grasps or misalignment during placement.</p>
            <div class="perception-overview">
                <div class="overview-content">
                    <h3>Challenge</h3>
                    <p>Detect yellow slide box and transparent glass slides using <strong>PURE RGB only!</strong></p>
                    <div class="challenge-box">
                        <div class="challenge-content">
                            <div class="challenge-text">
                                <p><strong>Input:</strong> An RGB image</p>
                                <p><strong>Output:</strong> Slide position + orientation (in SE3)</p>
                                <p><strong>Technical Challenge:</strong> Depth cam cannot see through transparent glass slide</p>
                            </div>
                            <div class="challenge-graphic">
                                <img src="images/intel_realsense-removebg-preview.png" alt="Perception sensor">
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <h3 class="subsection-title">Strategies</h3>
            <div class="strategies-grid">
                <div class="strategy-card">
                    <h4>ArUco Marker + OpenCV</h4>
                    <div class="strategy-content">
                        <div class="pipeline">
                            <div class="pipeline-step">Marker Detection</div>
                            <div class="pipeline-arrow">‚Üí</div>
                            <div class="pipeline-step">Box Localization</div>
                            <div class="pipeline-arrow">‚Üí</div>
                            <div class="pipeline-step">Slide Detection</div>
                            <div class="pipeline-arrow">‚Üí</div>
                            <div class="pipeline-step">Frame Publication</div>
                        </div>
                        <ul class="feature-list">
                            <li><strong>Marker Detection:</strong> Stable OpenCV library to locate the box & region of interest</li>
                            <li><strong>Output:</strong> SE3 of box (x-y position, depth, orientation)</li>
                            <li><strong>Slide Detection:</strong> Canny edge detection with 25 kernels for possible slots</li>
                            <li><strong>Performance:</strong> Very high reliability, fast processing</li>
                            <li><strong>Limitation:</strong> Requires marker, fixed slide position</li>
                        </ul>
                        <div class="method-images">
                            <img src="images/marker.jpg" alt="ArUco marker detection" class="method-image">
                        </div>
                    </div>
                </div>

                <div class="strategy-card">
                    <h4>G-SAM (GroundedDINO + SAM)</h4>
                    <div class="strategy-content">
                        <div class="pipeline">
                            <div class="pipeline-step">Text-Object Detection</div>
                            <div class="pipeline-arrow">‚Üí</div>
                            <div class="pipeline-step">Image Segmentation (SAM)</div>
                            <div class="pipeline-arrow">‚Üí</div>
                            <div class="pipeline-step">Slide Detection</div>
                            <div class="pipeline-arrow">‚Üí</div>
                            <div class="pipeline-step">Frame Publication</div>
                        </div>
                        <ul class="feature-list">
                            <li><strong>GroundedDINO:</strong> Text-based object detection for identifying objects</li>
                            <li><strong>SAM:</strong> Segment Anything for precise image segmentation</li>
                            <li><strong>Output:</strong> Object boxes and masked objects</li>
                            <li><strong>Performance:</strong> High reliability, slower processing</li>
                            <li><strong>Advantage:</strong> No marker required, adaptable to any slide position</li>
                        </ul>
                        <div class="method-images">
                            <img src="images/marker_free.jpg" alt="G-SAM detection pipeline" class="method-image">
                        </div>
                    </div>
                </div>
            </div>

            <div class="perception-visualization">
                <h3 class="subsection-title">Detection Visualization</h3>
                <div class="kernel-slider">
                    <div class="slider-header">
                    <span class="slider-label">Kernel Index</span>
                    <span class="slider-value" id="kernel-value">1</span>
                </div>
                <input type="range" id="kernel-slider" min="1" max="25" step="1" value="1">
                <div class="kernel-image-frame">
                    <img id="kernel-image" src="images/25_kernels/yellow_convolved_0.png" alt="Kernel frame">
                    <div id="glass-slide-indicator" class="glass-slide-indicator" style="display: none;">
                        ‚úì Glass Slide Detected
                    </div>
                </div>
            </div>
            </div>

            <div class="comparison-table">
                <h3 class="subsection-title">Method Comparison</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>ArUco Markers</th>
                            <th>G-SAM</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Speed</td>
                            <td class="positive">Fast</td>
                            <td>Slower</td>
                        </tr>
                        <tr>
                            <td>Reliability</td>
                            <td class="positive">Very High</td>
                            <td class="positive">High</td>
                        </tr>
                        <tr>
                            <td>Marker Required</td>
                            <td>Yes</td>
                            <td class="positive">No</td>
                        </tr>
                        <tr>
                            <td>Adaptability</td>
                            <td>Any slide position and orientation</td>
                            <td class="positive">Any slide position and orientation</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>
    </section>

    <!-- Planning Section -->
    <section id="planning" class="section dark-section">
        <div class="container">
            <h3 class="section-subhead">Planning</h3>
            <p class="section-note">The planning module takes the estimated slide pose from the perception stage and uses MoveIt2 to compute a feasible, collision-free motion plan for the robotic arm. This includes determining appropriate approach, grasp, and retreat motions while respecting kinematic constraints and environmental obstacles. The resulting trajectory ensures that the robot can safely and smoothly reach the target pose without violating task constraints.</p>

            <div class="step-flow">
                <div class="step-card">
                    <h4>1. Scan & Queue</h4>
                    <p>Parse TFs to build target list</p>
                </div>
                <div class="step-arrow">‚Üí</div>
                <div class="step-card">
                    <h4>2. Aligned Pick</h4>
                    <p>Match frame & grip</p>
                </div>
                <div class="step-arrow">‚Üí</div>
                <div class="step-card">
                    <h4>3. Move to Target</h4>
                    <p>Plan & transport</p>
                </div>
                <div class="step-arrow">‚Üí</div>
                <div class="step-card">
                    <h4>4. Tilt & Place</h4>
                    <p>Release with tilt</p>
                </div>
            </div>
            <div class="demo-grid">
                <div class="demo-item">
                    <img src="images/image39.gif" alt="Slide Detection Demo" class="demo-gif">
                </div>
                <div class="demo-item">
                    <img src="images/image40.gif" alt="Pick and Place Demo" class="demo-gif">
                </div>
                <div class="demo-item">
                    <img src="images/image38.gif" alt="Transport to target" class="demo-gif">
                </div>
                <div class="demo-item">
                    <img src="images/image41.gif" alt="Placement" class="demo-gif">
                </div>
            </div>

            <div class="planning-content">
                <h3>Computer Vision with MoveIt2</h3>
                <p>Integrating TM12 kinematics (link_6) for precise manipulation</p>

                <div class="planning-features">
                    <div class="feature-box">
                        <h4>ik.py Implementation</h4>
                        <ul class="feature-list">
                            <li><strong>Inverse Kinematics:</strong> MoveIt2 /compute_ik service for pose-to-joint conversion</li>
                            <li><strong>Trajectory Planning:</strong> RRTstar planner with scene objects and enforced joint limits</li>
                            <li><strong>Execution:</strong> /follow_joint_trajectory action client with dual velocity profiles
                                <ul>
                                    <li>Z-axis: 20% velocity</li>
                                    <li>XY-axes: 40% velocity</li>
                                </ul>
                            </li>
                            <li><strong>Technical Challenge:</strong> Quaternion normalization required for stable IK solutions</li>
                        </ul>
                    </div>

                    <div class="feature-box">
                        <h4>Key Capabilities</h4>
                        <ul class="feature-list">
                            <li>Pose ‚Üí target joint angles conversion</li>
                            <li>Collision avoidance</li>
                            <li>Stable after quaternion fix</li>
                            <li>Optimized path planning with RRTstar</li>
                        </ul>
                    </div>
                </div>
            </div>

        </div>
    </section>

    <!-- Actuation Section -->
    <section id="actuation" class="section">
        <div class="container">
            <h3 class="section-subhead">Actuation</h3>
            <p class="section-note">The actuation module executes the planned trajectory on the physical robot by sending motion commands to the arm and gripper. It coordinates joint-level control to achieve stable grasping and precise placement of the slide. Reliable actuation is essential for closing the loop between planning and real-world execution, ensuring that planned motions translate into consistent physical behavior. The gripper evolved from an initial MG996R servo design to an improved Arduino Nano‚Äìcontrolled gripper (USB to ROS2), providing more stable pick-and-place performance with the custom adapter.</p>

            <div class="actuation-challenge">
                <h3>Challenge</h3>
                <ul class="feature-list">
                    <li>No gripper available to use</li>
                    <li>Built-in camera unable to access when ROS2 listener node is running</li>
                </ul>
                <h3>Our Solution</h3>
                <p class="feature-list-item">Design a custom gripper to pick and place slides, and CAD an adapter to mount camera and gripper</p>
            </div>

            <div class="hardware-showcase"></div>

            <div class="hardware-grid">
                <div class="hardware-card">
                    <h4>Initial Gripper</h4>
                    <img src="images/C7FC0665-593E-4368-8C00-0ED90201FDCF.PNG" alt="Initial Gripper" class="card-image">
                    <ul class="feature-list">
                        <li><strong>Motor:</strong> MG996R servo motor (PWM-Controlled)</li>
                        <li><strong>Power:</strong> 4 AA battery holder (6V supply)</li>
                        <li><strong>Controller:</strong> Jetson Nano as ROS2 computer</li>
                    </ul>
                </div>

                <div class="hardware-card highlight">
                    <h4>Improved Gripper</h4>
                    <img src="images/image50.png" alt="Gripper control diagram" class="card-image">
                    <ul class="feature-list">
                        <li><strong>Enhanced Control:</strong> Arduino Nano mounted next to gripper</li>
                        <li><strong>Communication:</strong> USB (Serial) back to ROS2 computer</li>
                        <li><strong>Software:</strong> Gripper server in ROS2 + PWM Arduino code</li>
                    </ul>
                </div>
            </div>

            <div class="hardware-setup-wide">
                <div class="hardware-setup-media">
                    <img src="images/Image 2025-12-16 at 9.40‚ÄØPM.jpg" alt="Hardware Components" class="hardware-main-image">
                </div>
                <div class="hardware-setup-text">
                    <h3>Hardware Setup</h3>
                    <p>CAD and 3D-print an adapter to connect:</p>
                    <ul class="feature-list">
                        <li>Techman TM12M robot arm</li>
                        <li>Intel RealSense D435i depth camera</li>
                        <li>MG996R servo gripper</li>
                    </ul>
                </div>
            </div>

            <div class="adapter-iteration">
                <h3 class="subsection-title">Adapter Iteration</h3>
                <div class="iteration-grid">
                    <div class="iteration-card problem">
                        <h4>Version 1</h4>
                        <model-viewer class="model-viewer-frame"
                                      src="images/adaptor_v1.glb"
                                      auto-rotate
                                      camera-controls
                                      ar
                                      style="--progress-bar-color: var(--primary-color); --poster-color: transparent;"
                                      alt="Adapter Version 1 3D model">
                            <div class="model-fallback">
                                <p>Loading 3D model... ensure <code>model-viewer</code> script is available.</p>
                            </div>
                        </model-viewer>
                        <ul class="feature-list">
                            <li>Column structure failed after a few tests</li>
                            <li>Required a more robust structure</li>
                        </ul>
                    </div>
                    <div class="iteration-card solution">
                        <h4>Version 2</h4>
                        <model-viewer class="model-viewer-frame"
                                      src="images/adaptor_v6.glb"
                                      auto-rotate
                                      camera-controls
                                      ar
                                      style="--progress-bar-color: var(--primary-color); --poster-color: transparent;"
                                      alt="Adapter Version 2 3D model">
                            <div class="model-fallback">
                                <p>Loading 3D model... ensure <code>model-viewer</code> script is available.</p>
                            </div>
                        </model-viewer>
                        <ul class="feature-list">
                            <li>Higher infill rate to increase overall stiffness</li>
                            <li>Stronger ribs to distribute stress more equally</li>
                            <li>More suitable rib dimensions for simpler assembly</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Conclusion -->
    <section id="conclusion" class="section dark-section conclusion-section">
        <div class="container">
            <h2 class="section-title">Conclusion</h2>

            <h3 class="section-subhead">Summary</h3>
            <p class="section-note summary-note">This project delivers a full-stack, vision-guided pick-and-place system for fragile lab slides, integrating perception, planning, and actuation on a Techman arm with custom adapters and grippers. Robust perception (ArUco and G-SAM), MoveIt2-based planning, and iterative hardware refinements improved grasp stability and placement accuracy. The system demonstrates consistent end-to-end performance under real laboratory constraints, showing that careful sensing, trajectory generation, and execution can reliably automate slide handling.</p>
            <div class="summary-content">
                <a class="summary-item-link" href="#perception-target">
                    <div class="summary-item">
                        <div class="summary-number">01</div>
                        <p>Perception ‚Äî Implement G-SAM and slide edge detection pipeline to locate slides positions</p>
                    </div>
                </a>
                <a class="summary-item-link" href="#planning">
                    <div class="summary-item">
                        <div class="summary-number">02</div>
                        <p>Planning ‚Äî Utilize MoveIt2 to calculate inverse kinematics and generate path</p>
                    </div>
                </a>
                <a class="summary-item-link" href="#actuation">
                    <div class="summary-item">
                        <div class="summary-number">03</div>
                        <p>Actuation ‚Äî Connect hardware components with the custom adapter and execute control sequence</p>
                    </div>
                </a>
            </div>

            <h3 class="section-subhead">Future Work</h3>
            <p class="section-note">Future work focuses on four upgrades: a more robust CV model (e.g., end-to-end YOLO) to increase detection speed, improve accuracy, and reduce false positives; an adaptive gripper to handle a wider range of slide thicknesses and materials while maintaining a stable grasp; more precise placement to preserve orientation and spacing, preventing downstream alignment issues; and speed optimization via precomputed paths plus performance monitoring to cut cycle time and surface anomalies early. Together, these enhancements aim to make the system faster, more reliable, and easier to deploy across diverse lab workflows.</p>
            <div class="future-grid">
                <div class="future-card">
                    <div class="future-icon">üéØ</div>
                    <h4>Robust CV Model</h4>
                    <p>End-to-end YOLO model after dataset collection</p>
                </div>
                <div class="future-card">
                    <div class="future-icon">ü§è</div>
                    <h4>Adaptive Gripper</h4>
                    <p>Enhanced gripper design for various slide types</p>
                </div>
                <div class="future-card">
                    <div class="future-icon">üìä</div>
                    <h4>Precise Placement</h4>
                    <p>Accurate placement in sorted order</p>
                </div>
                <div class="future-card">
                    <div class="future-icon">‚ö°</div>
                    <h4>Speed Optimization</h4>
                    <p>Improve throughput by precomputing paths</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Team Section -->
    <section id="team" class="section">
        <div class="container">
            <div class="team-info">
                <h3>Team 45</h3>
                <div class="team-members">
                    <div class="member">
                        <a class="member-link" href="https://www.linkedin.com/in/twchen1258?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=ios_app" target="_blank" rel="noopener noreferrer">David Chen</a>
                        <div class="member-role">Actuation</div>
                    </div>
                    <div class="member">
                        <a class="member-link" href="https://www.linkedin.com/in/yu-chen-ryan-chung-6023761b4?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=ios_app" target="_blank" rel="noopener noreferrer">Ryan Chung</a>
                        <div class="member-role">Website, Support</div>
                    </div>
                    <div class="member">
                        <a class="member-link" href="https://www.linkedin.com/in/%E8%82%B2%E7%91%8B-%E5%BC%B5-6005ab2b8?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=ios_app" target="_blank" rel="noopener noreferrer">Yu-Wei Chang</a>
                        <div class="member-role">CAD, Support</div>
                    </div>
                    <div class="member">
                        <a class="member-link" href="https://www.linkedin.com/in/bryanchang9?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=ios_app" target="_blank" rel="noopener noreferrer">Bryan Chang</a>
                        <div class="member-role">Planning</div>
                    </div>
                    <div class="member">
                        <a class="member-link" href="https://www.linkedin.com/in/kain-yukai?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=ios_app" target="_blank" rel="noopener noreferrer">Kain Hung</a>
                        <div class="member-role">Perception</div>
                    </div>
                </div>
                <div class="team-photo">
                    <img src="images/A06A3410-07AA-4A2C-9B80-09E02DA154F3.PNG" alt="Team 45 group photo">
                </div>
                <div class="sponsor-section">
                    <h4>Industry Sponsor</h4>
                    <div class="sponsor-inline">
                        <a class="sponsor-link" href="https://www.emberrobotics.com/?fbclid=PAVERFWAOwW2tleHRuA2FlbQIxMABzcnRjBmFwcF9pZA8xMjQwMjQ1NzQyODc0MTQAAafVPpRGxLzPFImMw6ngKpcwA2eBrGzsqKDoDbis0eEX3I6csSywF9z_8BO5AA_aem_jpWe8Ew0_2x6j5XCeodBFw" target="_blank" rel="noopener noreferrer">
                            <img src="images/image19.png" alt="Ember Robotics logo" class="sponsor-logo">
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer>
        <div class="container">
            <p>&copy; 2025 PRISM Project - Team 45 | EECS/ME 206A</p>
        </div>
    </footer>

    <script type="module" src="libs/model-viewer.min.js"></script>
    <script src="script.js"></script>
</body>
</html>
