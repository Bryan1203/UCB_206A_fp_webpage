<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PRISM: Precision Relocation via Intelligent Slide Manipulation</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <!-- Navigation -->
    <nav id="navbar">
        <div class="nav-container">
            <div class="logo">PRISM</div>
            <ul class="nav-menu">
                <li><a href="#introduction">Introduction</a></li>
                <li><a href="#results">Results</a></li>
                <li><a href="#design">Design</a></li>
                <li><a href="#implementation">Implementation</a></li>
                <li><a href="#conclusion">Conclusion</a></li>
                <li><a href="#team">Team</a></li>
                <li><a href="#additional-materials">Additional Materials</a></li>
            </ul>
        </div>
    </nav>

    <!-- Hero Section -->
    <section id="home" class="hero">
        <div class="hero-content">
            <h1 class="glitch" data-text="PRISM">PRISM</h1>
            <h2>Precision Relocation via Intelligent Slide Manipulation</h2>
            <p class="subtitle">An intelligent robotic system for automated slide handling</p>
            <div class="hero-meta">
                <span class="course">EECS/ME 206A - Team 45</span>
                <span class="separator">|</span>
                <span class="sponsor">Industry Project by Ember Robotics</span>
            </div>
            <a href="#introduction" class="cta-button">Explore Project</a>
        </div>
    </section>

    <!-- Problem Statement -->
    <section id="introduction" class="section problem-section">
        <div class="container">
            <h2 class="section-title">Introduction</h2>
            <p class="section-note">This project is an Industry Track collaboration with Ember Robotics to design and implement a vision-guided pick-and-place system using a Techman robotic arm. Our goal is to manipulate fragile glass lab slides with high precision, which requires robust object detection, accurate motion planning, and precise actuation. This work addresses critical challenges in places such as laboratory environments or chemical handling manufacturing lines, where current workflows suffer from:</p>
            <div class="problem-grid">
                <div class="problem-card">
                    <div class="icon">üë∑‚Äç‚ôÇÔ∏è</div>
                    <h3>Manual & Inefficient</h3>
                    <p>Current slide handling workflow requires manual intervention</p>
                </div>
                <div class="problem-card">
                    <div class="icon">ü¶†</div>
                    <h3>Contamination Risks</h3>
                    <p>Potential contamination risks that affect experiment results</p>
                </div>
            </div>

            <div class="goal-section">
                <div class="goal-inner">
                    <div class="goal-content">
                        <div class="goal-text">
                            <h3>Our Goal</h3>
                            <p>We designed, built, and tested a vision-guided pick-and-place system using a Techman TM12 robotic arm that can manipulate and relocate lab glass slides with high precision. For successful task execution, the robot detects the position and orientation of each slide, picks it from a source tray, and places it accurately onto a target rack. This project tackles several interesting technical problems that make automated slide manipulation difficult:</p>
                            <ul class="feature-list">
                                <li>Computer vision for slide identification</li>
                                <li>Precise pick-and-place in 3D space</li>
                                <li>Robust operation for continuous tasks</li>
                            </ul>
                        </div>
                        <div class="goal-graphic">
                            <img src="images/image23.png" alt="PRISM goal illustration">
                        </div>
                    </div>
                </div>
                <p class="approach">Our approach combines computer vision, motion planning, and custom hardware into a complete ROS2 system. We detect slide poses, compute precise trajectories, and execute robust pick-and-place actions with a custom adapter and gripper, targeting repeatable performance in real-world laboratory settings.</p>
            </div>

        </div>
    </section>

    <!-- Demo Video -->
    <section id="results" class="section">
        <div class="container">
            <h2 class="section-title">Results</h2>
            <p class="section-note">The final system demonstrates consistent and repeatable slide manipulation across multiple trials, as shown in the visual demonstrations below.</p>
            <div class="video-wrapper">
                <iframe src="https://www.youtube.com/embed/4Mbxl2Z15pg?si=uDb3a2DtugLRp3_x"
                        title="PRISM Demo Video"
                        frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                        allowfullscreen>
                </iframe>
            </div>
        </div>
    </section>

    <!-- System Overview -->
    <section id="design" class="section dark-section">
        <div class="container">
            <h2 class="section-title">Design</h2>
            <h3 class="section-subhead">Design Criteria &amp; Requirements</h3>
            <h4>Functional Requirements:</h4>
            <ul class="feature-list">
                <li>Detect and localize transparent glass slides with &lt;5mm position error</li>
                <li>Pick slides without damage or slippage</li>
                <li>Place slides accurately in target rack</li>
                <li>Execute pick and place cycles continuously without failure</li>
            </ul>
            <h4>System Constraints:</h4>
            <ul class="feature-list">
                <li>Must integrate with existing TM12M robot arm</li>
                <li>No existing gripper available to use</li>
                <li>Must be able to pick up standard size lab slides (25mm x 75mm x 1mm)</li>
                <li>RGB-only vision (depth sensors fail on transparent objects)</li>
                <li>Laboratory space constraints</li>
            </ul>
            <h3 class="section-subhead">Design Overview</h3>
            <p class="section-note">The system is designed as a modular pipeline consisting of perception, planning, and actuation components. Hardware design focuses on a custom adapter to ensure mechanical stability, while software design emphasizes clear separation between sensing and execution through iterative refinement. This architecture separates concerns, enabling independent development and testing of each module while maintaining clear interfaces through ROS2 topics and services.</p>
            <div class="system-pillars">
                <a class="pillar-link" href="#perception-target">
                    <div class="pillar">
                        <div class="pillar-icon">üëÅÔ∏è</div>
                        <h3>Perception</h3>
                        <p>Computer Vision algorithm to detect slides from RGB image and estimate their 6D poses</p>
                    </div>
                </a>
                <a class="pillar-link" href="#planning">
                    <div class="pillar">
                        <div class="pillar-icon">üß†</div>
                        <h3>Planning</h3>
                        <p>Motion planning with MoveIt2, leveraging OMPL for path-finding and IK solvers</p>
                    </div>
                </a>
                <a class="pillar-link" href="#actuation">
                    <div class="pillar">
                        <div class="pillar-icon">‚öôÔ∏è</div>
                        <h3>Actuation</h3>
                        <p>Custom hardware interface executing trajectories via ROS2 services on microcontroller-driven gripper</p>
                    </div>
                </a>
            </div>

        </div>
    </section>

    <!-- System Architecture -->
    <section id="architecture" class="section dark-section">
        <div class="container">
            <h2 class="section-title">System Architecture</h2>
            <p class="section-note">The system is designed to be launched via a centralized launch file that initializes all necessary ROS2 nodes. Once ready, the user sends a service request to the pick_and_place node, which orchestrates the entire manipulation pipeline. This node calls the slide_detector helper node to obtain slide information. The slide_detector subscribes to detected slide transforms (TFs) from either marker_detect or gsam_detect nodes and maintains a queue of slides to be picked. A static_tf_broadcaster connects the camera frame to the robot's TF tree, establishing the spatial reference. Finally, the ik_node computes inverse kinematics and generates optimal collision-free paths using MoveIt2, while the gripper_server node controls the servo actuator via Arduino interface.</p>

            <div class="architecture-image-container">
                <img src="images/42A5B40D-EB54-408C-9088-F089410A0A6A.PNG" alt="PRISM System Architecture" class="architecture-main-image">
            </div>

            <div class="design-table-wrapper">
                <h3 class="section-subhead">Key Design Choices &amp; Trade-offs</h3>
                <div class="design-table">
                    <table>
                        <thead>
                            <tr>
                                <th>Design Choice</th>
                                <th>Rationale</th>
                                <th>Trade-off</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Dual perception methods (ArUco + G-SAM)</td>
                                <td>ArUco is fast but needs markers; G-SAM is flexible but slower</td>
                                <td>Speed vs. adaptability. Therefore, we provide both options for the users to choose</td>
                            </tr>
                            <tr>
                                <td>Intel RealSense D435i camera</td>
                                <td>Camera already available for us; small and lightweight; Better resolution and frame rate than built-in camera</td>
                                <td>Additional hardware to mount and wire, but saves development time and provides the quality needed for precise slide detection</td>
                            </tr>
                            <tr>
                                <td>MoveIt2 + RRTstar planner</td>
                                <td>Proven collision avoidance, handles complex joint constraints; team members familiar with this library</td>
                                <td>Slower than analytical IK, but more robust for obstacles</td>
                            </tr>
                            <tr>
                                <td>Custom adapter (v2 with ribs)</td>
                                <td>No off-the-shelf mounts could fit camera + gripper together in desired location and orientation</td>
                                <td>Added design/fabrication time (especially multiple iterations), but customizable to our exact needs</td>
                            </tr>
                            <tr>
                                <td>Arduino-controlled gripper</td>
                                <td>Jetson Nano PWM was unstable over long distances; offloading to Arduino Nano improved reliability</td>
                                <td>Extra hardware component and cable, but cleaner ROS2 interface</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <h3 class="section-subhead">Impact on Real World Engineering Criteria</h3>
            <p class="section-note">These design choices directly improve real-world performance: Robustness is achieved through dual perception methods (ArUco and G-SAM) that provide fallback options when markers are unavailable or slides are repositioned, plus the MoveIt2 IK solver that prevents trajectory planning failures. Durability comes from the reinforced adapter v2 that survived 20+ test cycles without cracking, and the careful selection of wire gauges and WAGO connectors that reliably power the gripper. Efficiency is demonstrated by ArUco detection running at 30 fps for real-time operation, and the Arduino-controlled gripper achieves minimal latency because it runs dedicated bare-metal code. The modular ROS2 architecture also allows parallel development and easy component swapping, reducing integration time during testing and debugging.</p>

        </div>
    </section>

    <!-- Perception Section -->
    <section id="implementation" class="section">
        <div class="container">
            <h2 class="section-title">Implementation</h2>
            <p class="section-note">The implementation integrates custom hardware with a software pipeline for perception, motion planning, and execution, enabling reliable end-to-end slide manipulation.</p>

            <div id="perception-target" class="anchor-spacer"></div>
            <h3 class="section-subhead">Perception</h3>
            <p class="section-note">The perception module is responsible for detecting the microscope slide and estimating its pose within the workspace. Using Intel RealSense input, the system identifies the slide‚Äôs position and orientation relative to the robot with CV algorithm, providing a consistent geometric reference for downstream modules. Accurate perception is critical for reliable grasping, as small errors in pose estimation can lead to failed grasps or misalignment during placement.</p>
            <div class="perception-overview">
                <div class="overview-content">
                    <h3>Challenge</h3>
                    <p>Detect yellow slide box and transparent glass slides using <strong>PURE RGB only!</strong></p>
                    <div class="challenge-box">
                        <div class="challenge-content">
                            <div class="challenge-text">
                                <p><strong>Input:</strong> An RGB image</p>
                                <p><strong>Output:</strong> Slide position + orientation (in SE3)</p>
                                <p><strong>Technical Challenge:</strong> Depth cam cannot see through transparent glass slide</p>
                            </div>
                            <div class="challenge-graphic">
                                <img src="images/intel_realsense-removebg-preview.png" alt="Perception sensor">
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <h3 class="subsection-title">Strategies</h3>
            <div class="strategies-grid">
                <div class="strategy-card">
                    <h4>ArUco Marker + OpenCV</h4>
                    <div class="strategy-content">
                        <div class="pipeline">
                            <div class="pipeline-step">Marker Detect</div>
                            <div class="pipeline-arrow">‚Üí</div>
                            <div class="pipeline-step">Box Localization</div>
                            <div class="pipeline-arrow">‚Üí</div>
                            <div class="pipeline-step">Slide Detection</div>
                            <div class="pipeline-arrow">‚Üí</div>
                            <div class="pipeline-step">Frame Publication</div>
                        </div>
                        <ul class="feature-list">
                            <li><strong>Marker Detection:</strong> Stable OpenCV library to locate the box & region of interest</li>
                            <li><strong>Output:</strong> SE3 of box (x-y position, depth, orientation)</li>
                            <li><strong>Slide Detection:</strong> Canny edge detection with 25 kernels for possible slots</li>
                            <li><strong>Performance:</strong> Very high reliability, fast processing</li>
                            <li><strong>Limitation:</strong> Requires marker, fixed slide position</li>
                        </ul>
                        <div class="method-images">
                            <img src="images/marker.jpg" alt="ArUco marker detection" class="method-image">
                        </div>
                    </div>
                </div>

                <div class="strategy-card">
                    <h4>G-SAM (GroundedDINO + SAM)</h4>
                    <div class="strategy-content">
                        <div class="pipeline">
                            <div class="pipeline-step">Text-Object Detection</div>
                            <div class="pipeline-arrow">‚Üí</div>
                            <div class="pipeline-step">Image Segmentation (SAM)</div>
                            <div class="pipeline-arrow">‚Üí</div>
                            <div class="pipeline-step">Slide Detection</div>
                            <div class="pipeline-arrow">‚Üí</div>
                            <div class="pipeline-step">Frame Publication</div>
                        </div>
                        <ul class="feature-list">
                            <li><strong>GroundedDINO:</strong> Text-based object detection for identifying objects</li>
                            <li><strong>SAM:</strong> Segment Anything for precise image segmentation</li>
                            <li><strong>Output:</strong> Object boxes and masked objects</li>
                            <li><strong>Performance:</strong> High reliability, slower processing</li>
                            <li><strong>Advantage:</strong> No marker required, adaptable to any slide position</li>
                        </ul>
                        <div class="method-images">
                            <img src="images/marker_free.jpg" alt="G-SAM detection pipeline" class="method-image">
                        </div>
                    </div>
                </div>
            </div>

            <div class="perception-visualization">
                <h3 class="subsection-title">Detection Visualization</h3>
                <div class="kernel-slider">
                    <div class="slider-header">
                    <span class="slider-label">Kernel Index</span>
                    <span class="slider-value" id="kernel-value">1</span>
                </div>
                <input type="range" id="kernel-slider" min="1" max="25" step="1" value="1">
                <div class="kernel-image-frame">
                    <img id="kernel-image" src="images/25_kernels/yellow_convolved_0.png" alt="Kernel frame">
                </div>
            </div>
            </div>

            <div class="comparison-table">
                <h3 class="subsection-title">Method Comparison</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>ArUco Markers</th>
                            <th>G-SAM</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Speed</td>
                            <td class="positive">Fast</td>
                            <td>Slower</td>
                        </tr>
                        <tr>
                            <td>Reliability</td>
                            <td class="positive">Very High</td>
                            <td class="positive">High</td>
                        </tr>
                        <tr>
                            <td>Marker Required</td>
                            <td>Yes</td>
                            <td class="positive">No</td>
                        </tr>
                        <tr>
                            <td>Adaptability</td>
                            <td>Any slide position and orientation</td>
                            <td class="positive">Any slide position and orientation</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>
    </section>

    <!-- Planning Section -->
    <section id="planning" class="section dark-section">
        <div class="container">
            <h3 class="section-subhead">Planning</h3>
            <p class="section-note">The planning module takes the estimated slide pose from the perception stage and uses MoveIt2 to compute a feasible, collision-free motion plan for the robotic arm. This includes determining appropriate approach, grasp, and retreat motions while respecting kinematic constraints and environmental obstacles. The resulting trajectory ensures that the robot can safely and smoothly reach the target pose without violating task constraints.</p>

            <div class="step-flow">
                <div class="step-card">
                    <h4>1. Scan & Queue</h4>
                    <p>Parse TFs to build target list</p>
                </div>
                <div class="step-arrow">‚Üí</div>
                <div class="step-card">
                    <h4>2. Aligned Pick</h4>
                    <p>Match frame & grip</p>
                </div>
                <div class="step-arrow">‚Üí</div>
                <div class="step-card">
                    <h4>3. Move to Target</h4>
                    <p>Plan & transport</p>
                </div>
                <div class="step-arrow">‚Üí</div>
                <div class="step-card">
                    <h4>4. Tilt & Place</h4>
                    <p>Release with tilt</p>
                </div>
            </div>
            <div class="demo-grid">
                <div class="demo-item">
                    <img src="images/image39.gif" alt="Slide Detection Demo" class="demo-gif">
                </div>
                <div class="demo-item">
                    <img src="images/image40.gif" alt="Pick and Place Demo" class="demo-gif">
                </div>
                <div class="demo-item">
                    <img src="images/image38.gif" alt="Transport to target" class="demo-gif">
                </div>
                <div class="demo-item">
                    <img src="images/image41.gif" alt="Placement" class="demo-gif">
                </div>
            </div>

            <div class="planning-content">
                <h3>Computer Vision with MoveIt2</h3>
                <p>Integrating TM12 kinematics (link_6) for precise manipulation</p>

                <div class="planning-features">
                    <div class="feature-box">
                        <h4>ik.py Implementation</h4>
                        <ul class="feature-list">
                            <li><strong>Inverse Kinematics:</strong> MoveIt2 /compute_ik service for pose-to-joint conversion</li>
                            <li><strong>Trajectory Planning:</strong> RRTstar planner with scene objects and enforced joint limits</li>
                            <li><strong>Execution:</strong> /follow_joint_trajectory action client with dual velocity profiles
                                <ul>
                                    <li>Z-axis: 20% velocity</li>
                                    <li>XY-axes: 40% velocity</li>
                                </ul>
                            </li>
                            <li><strong>Technical Challenge:</strong> Quaternion normalization required for stable IK solutions</li>
                        </ul>
                    </div>

                    <div class="feature-box">
                        <h4>Key Capabilities</h4>
                        <ul class="feature-list">
                            <li>Pose ‚Üí target joint angles conversion</li>
                            <li>Collision avoidance</li>
                            <li>Stable after quaternion fix</li>
                            <li>Optimized path planning with RRTstar</li>
                        </ul>
                    </div>
                </div>
            </div>

        </div>
    </section>

    <!-- Actuation Section -->
    <section id="actuation" class="section">
        <div class="container">
            <h3 class="section-subhead">Actuation</h3>
            <p class="section-note">The actuation module executes the planned trajectory on the physical robot by sending motion commands to the arm and gripper. It coordinates joint-level control to achieve stable grasping and precise placement of the slide. Reliable actuation is essential for closing the loop between planning and real-world execution, ensuring that planned motions translate into consistent physical behavior. The gripper evolved from an initial MG996R servo design to an improved Arduino Nano‚Äìcontrolled gripper (USB to ROS2), providing more stable pick-and-place performance with the custom adapter.</p>

            <div class="actuation-challenge">
                <h3>Challenge</h3>
                <ul class="feature-list">
                    <li>No gripper available to use</li>
                    <li>Built-in camera unable to access when ROS2 listener node is running</li>
                </ul>
                <h3>Our Solution</h3>
                <p class="feature-list-item">Design a custom gripper to pick and place slides, and CAD an adapter to mount camera and gripper</p>
            </div>

            <div class="hardware-showcase"></div>

            <div class="hardware-grid">
                <div class="hardware-card">
                    <h4>Initial Gripper</h4>
                    <img src="images/C7FC0665-593E-4368-8C00-0ED90201FDCF.PNG" alt="Initial Gripper" class="card-image">
                    <ul class="feature-list">
                        <li><strong>Motor:</strong> MG996R servo motor (PWM-Controlled)</li>
                        <li><strong>Power:</strong> 4 AA battery holder (6V supply)</li>
                        <li><strong>Controller:</strong> Jetson Nano as ROS2 computer</li>
                    </ul>
                </div>

                <div class="hardware-card highlight">
                    <h4>Improved Gripper</h4>
                    <img src="images/image50.png" alt="Gripper control diagram" class="card-image">
                    <ul class="feature-list">
                        <li><strong>Enhanced Control:</strong> Arduino Nano mounted next to gripper</li>
                        <li><strong>Communication:</strong> USB (Serial) back to ROS2 computer</li>
                        <li><strong>Software:</strong> Gripper server in ROS2 + PWM Arduino code</li>
                    </ul>
                </div>
            </div>

            <div class="hardware-setup-wide">
                <div class="hardware-setup-media">
                    <img src="images/Image 2025-12-16 at 9.40‚ÄØPM.jpg" alt="Hardware Components" class="hardware-main-image">
                </div>
                <div class="hardware-setup-text">
                    <h3>Hardware Setup</h3>
                    <p>CAD and 3D-print an adapter to connect:</p>
                    <ul class="feature-list">
                        <li>Techman TM12M robot arm</li>
                        <li>Intel RealSense D435i depth camera</li>
                        <li>MG996R servo gripper</li>
                    </ul>
                </div>
            </div>

            <div class="adapter-iteration">
                <h3 class="subsection-title">Adapter Iteration</h3>
                <div class="iteration-grid">
                    <div class="iteration-card problem">
                        <h4>Version 1</h4>
                        <model-viewer class="model-viewer-frame"
                                      src="images/adaptor_v1.glb"
                                      auto-rotate
                                      camera-controls
                                      ar
                                      style="--progress-bar-color: var(--primary-color); --poster-color: transparent;"
                                      alt="Adapter Version 1 3D model">
                            <div class="model-fallback">
                                <p>Loading 3D model... ensure <code>model-viewer</code> script is available.</p>
                            </div>
                        </model-viewer>
                        <ul class="feature-list">
                            <li>Column structure failed after a few tests</li>
                            <li>Required a more robust structure</li>
                        </ul>
                    </div>
                    <div class="iteration-card solution">
                        <h4>Version 2</h4>
                        <model-viewer class="model-viewer-frame"
                                      src="images/adaptor_v6.glb"
                                      auto-rotate
                                      camera-controls
                                      ar
                                      style="--progress-bar-color: var(--primary-color); --poster-color: transparent;"
                                      alt="Adapter Version 2 3D model">
                            <div class="model-fallback">
                                <p>Loading 3D model... ensure <code>model-viewer</code> script is available.</p>
                            </div>
                        </model-viewer>
                        <ul class="feature-list">
                            <li>Higher infill rate to increase overall stiffness</li>
                            <li>Stronger ribs to distribute stress more equally</li>
                            <li>More suitable rib dimensions for simpler assembly</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Conclusion -->
    <section id="conclusion" class="section dark-section conclusion-section">
        <div class="container">
            <h2 class="section-title">Conclusion</h2>

            <h3 class="section-subhead">Summary</h3>
            <p class="section-note summary-note">This project delivers a full-stack, vision-guided pick-and-place system for fragile lab slides, integrating perception, planning, and actuation on a Techman arm with custom adapters and grippers. Robust perception (ArUco and G-SAM), MoveIt2-based planning, and iterative hardware refinements improved grasp stability and placement accuracy. The system demonstrates consistent end-to-end performance under real laboratory constraints, showing that careful sensing, trajectory generation, and execution can reliably automate slide handling.</p>
            <div class="summary-content">
                <a class="summary-item-link" href="#perception-target">
                    <div class="summary-item">
                        <div class="summary-number">01</div>
                        <p>Perception ‚Äî Implement G-SAM and slide edge detection pipeline to locate slides positions</p>
                    </div>
                </a>
                <a class="summary-item-link" href="#planning">
                    <div class="summary-item">
                        <div class="summary-number">02</div>
                        <p>Planning ‚Äî Utilize MoveIt2 to calculate inverse kinematics and generate path</p>
                    </div>
                </a>
                <a class="summary-item-link" href="#actuation">
                    <div class="summary-item">
                        <div class="summary-number">03</div>
                        <p>Actuation ‚Äî Connect hardware components with the custom adapter and execute control sequence</p>
                    </div>
                </a>
            </div>

            <h3 class="section-subhead">Future Work</h3>
            <p class="section-note">Future work focuses on four upgrades: a more robust CV model (e.g., end-to-end YOLO) to increase detection speed, improve accuracy, and reduce false positives; an adaptive gripper to handle a wider range of slide thicknesses and materials while maintaining a stable grasp; more precise placement to preserve orientation and spacing, preventing downstream alignment issues; and speed optimization via precomputed paths plus performance monitoring to cut cycle time and surface anomalies early. Together, these enhancements aim to make the system faster, more reliable, and easier to deploy across diverse lab workflows.</p>
            <div class="future-grid">
                <div class="future-card">
                    <div class="future-icon">üéØ</div>
                    <h4>Robust CV Model</h4>
                    <p>End-to-end YOLO model after dataset collection</p>
                </div>
                <div class="future-card">
                    <div class="future-icon">ü§è</div>
                    <h4>Adaptive Gripper</h4>
                    <p>Enhanced gripper design for various slide types</p>
                </div>
                <div class="future-card">
                    <div class="future-icon">üìä</div>
                    <h4>Precise Placement</h4>
                    <p>Accurate placement in sorted order</p>
                </div>
                <div class="future-card">
                    <div class="future-icon">‚ö°</div>
                    <h4>Speed Optimization</h4>
                    <p>Improve throughput by precomputing paths</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Team Section -->
    <section id="team" class="section">
        <div class="container">
            <div class="team-info">
                <h3>Team 45</h3>

                <div class="team-table">
                    <table>
                        <thead>
                            <tr>
                                <th>Name</th>
                                <th>Contribution</th>
                                <th>Background</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><a class="team-link" href="https://www.linkedin.com/in/twchen1258?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=ios_app" target="_blank" rel="noopener noreferrer">David Chen</a></td>
                                <td>Actuation</td>
                                <td>David is an EECS M.Eng. student specializing in Robotics and AI. He completed his undergraduate studies in Computer Science and Engineering at UC Irvine, where he worked on various embedded software projects involving object detection algorithms, neural networks, Raspberry Pi, Arduino, and drones. He also has hands-on experience in vehicle engineering and technical project management through his role as the Project Manager of the UCI Solar Car Project, where he led a multidisciplinary team in building a fully solar-powered electric vehicle from the ground up in two years.</td>
                            </tr>
                            <tr>
                                <td><a class="team-link" href="https://www.linkedin.com/in/yu-chen-ryan-chung-6023761b4?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=ios_app" target="_blank" rel="noopener noreferrer">Ryan Chung</a></td>
                                <td>Website, Support</td>
                                <td>Ryan is an M.Eng. student in Mechanical Engineering at UC Berkeley, focusing on robotics and control. He completed his undergraduate degree in Mechanical Engineering at National Taiwan University, with a minor in Music specializing in piano performance from National Taiwan Normal University. His academic and research interests center on medical and rehabilitation robotics, where he aims to develop systems that support healthcare professionals and improve patient care. His background in piano performance offers a distinctive perspective for the team‚Äôs robotic pianist project.</td>
                            </tr>
                            <tr>
                                <td><a class="team-link" href="https://www.linkedin.com/in/%E8%82%B2%E7%91%8B-%E5%BC%B5-6005ab2b8?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=ios_app" target="_blank" rel="noopener noreferrer">Yu-Wei Chang</a></td>
                                <td>CAD, Support</td>
                                <td>Yu-Wei is an M.Eng. student in Mechanical Engineering at UC Berkeley with a high interest in robotics, control, and mechanical design. He completed his undergraduate degree in Power Mechanical Engineering at National Tsing Hua University, where he worked on various projects such as a DC motor with a PI controller, Arduino applications, and mechanical structure design. His internship experiences at two manufacturing companies gave him great insight into industry needs.</td>
                            </tr>
                            <tr>
                                <td><a class="team-link" href="https://www.linkedin.com/in/bryanchang9?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=ios_app" target="_blank" rel="noopener noreferrer">Bryan Chang</a></td>
                                <td>Planning</td>
                                <td>Bryan is a MEng student in Electrical Engineering and Computer Science at UC Berkeley, specializing in autonomous driving systems and sensor data processing. With hands-on experience from multiple internships in the autonomous vehicle industry, he has developed expertise in LiDAR perception systems and created transformer-based machine learning models to automate lane line labeling for 3D point clouds. Currently working on an autonomous racing car project, Bryan is advancing LiDAR localization systems that operate under high-performance, real-time constraints. His technical foundation spans computer vision, signal processing, and robotics, with a particular strength in transforming raw sensor data into reliable, safety-critical insights for autonomous vehicles. Bryan's passion lies in solving the complex challenges of perception and localization that enable autonomous systems to navigate safely in dynamic environments.</td>
                            </tr>
                            <tr>
                                <td><a class="team-link" href="https://www.linkedin.com/in/kain-yukai?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=ios_app" target="_blank" rel="noopener noreferrer">Kain Hung</a></td>
                                <td>Perception</td>
                                <td>Kain is an EECS M.Eng. student with broad enthusiasm in computer vision, robotics, and AI. He completed his undergraduate degree at National Taiwan University, Computer Science department with three Human-Computer Interaction projects, where he explored numerous skills in hardware and software co-designing, including Arduino, OpenCV, OpenPose, and Pytorch. His hands-on experience includes a large-scale tangible interface, a thin, card-shaped robot with ML-based perception, and a multimodal AI system.</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                <div class="team-photo">
                    <img src="images/A06A3410-07AA-4A2C-9B80-09E02DA154F3.PNG" alt="Team 45 group photo">
                </div>
                <div class="sponsor-section">
                    <h4>Industry Sponsor</h4>
                    <div class="sponsor-inline">
                        <a class="sponsor-link" href="https://www.emberrobotics.com/?fbclid=PAVERFWAOwW2tleHRuA2FlbQIxMABzcnRjBmFwcF9pZA8xMjQwMjQ1NzQyODc0MTQAAafVPpRGxLzPFImMw6ngKpcwA2eBrGzsqKDoDbis0eEX3I6csSywF9z_8BO5AA_aem_jpWe8Ew0_2x6j5XCeodBFw" target="_blank" rel="noopener noreferrer">
                            <img src="images/image19.png" alt="Ember Robotics logo" class="sponsor-logo">
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Additional Materials -->
    <section id="additional-materials" class="section">
        <div class="container">
            <h2 class="section-title">Additional Materials</h2>
            <p class="section-note">Link to our GitHub page: <a href="https://github.com/YuKai0928/25F_206A_FinalProject" target="_blank" rel="noopener noreferrer">https://github.com/YuKai0928/25F_206A_FinalProject</a></p>
            <ul class="feature-list">
                <li>ROS2 packages, launch files, model, and instructions</li>
            </ul>
            <p class="section-note">Link to our Google Drive: <a href="https://drive.google.com/drive/folders/1J0ep10ODH-zIUo68zBBcHVBDpZjkjJoT?usp=sharing" target="_blank" rel="noopener noreferrer">https://drive.google.com/drive/folders/1J0ep10ODH-zIUo68zBBcHVBDpZjkjJoT?usp=sharing</a></p>
            <ul class="feature-list">
                <li>CAD models for adapters</li>
                <li>Component datasheets</li>
                <li>Additional videos and images</li>
            </ul>
        </div>
    </section>

    <!-- Footer -->
    <footer>
        <div class="container">
            <p>&copy; 2025 PRISM Project - Team 45 | EECS/ME 206A</p>
        </div>
    </footer>

    <script type="module" src="libs/model-viewer.min.js"></script>
    <script src="script.js"></script>
</body>
</html>
